{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from module.prepare import *\n",
    "from itertools import product\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "from itertools import islice\n",
    "import time\n",
    "import configparser\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "\n",
    "import sklearn\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.manifold import *\n",
    "import sklearn.tree as Tr \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import optuna.integration.lightgbm\n",
    "\n",
    "from module.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentTime():\n",
    "    return datetime.datetime.strftime(datetime.datetime.fromtimestamp(time.time()),format='%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "\n",
    "def LGBOptuna(trial):\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    " \n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    " \n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(test_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_y, pred_labels)\n",
    "    return accuracy\n",
    " \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)\n",
    "    return\n",
    "\n",
    "def LGBTuning(Xtrain,Xtest,Ytrain,Ytest,new_params=None):\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(objective='cross_entropy', ### {cross_entropy, binary}\n",
    "#                              silent=False,\n",
    "                             verbose=0,\n",
    "                             random_state=seed,\n",
    "                             n_jobs=20,\n",
    "#                              class_weight\n",
    "                            )\n",
    "    \n",
    "    default_params = {\n",
    "    'learning_rate': [0.1], \n",
    "    'boosting_type':['gbdt'], \n",
    "    'n_estimators': [500],\n",
    "    'num_iterations':[1000],\n",
    "    'max_bin':[256]\n",
    "    }\n",
    "    \n",
    "    if new_params is not None:\n",
    "        default_params.update(new_params)\n",
    "    \n",
    "    arg_str = ''\n",
    "    for k,v in default_params.items():\n",
    "        if type(v[0])==str:\n",
    "            arg_str += k+'='+\"'\"+v[0]+\"',\"\n",
    "        else:\n",
    "            arg_str += k+'='+str(v[0])+\",\"\n",
    "    eval(\n",
    "        'clf.'+clf.set_params.__name__+\"(\"\n",
    "            +arg_str.rstrip(',')+\n",
    "            \")\"\n",
    "        )\n",
    "\n",
    "#     print('DEBUG:: tuning params\\n',clf.get_params())\n",
    "    clf.fit(Xtrain,Ytrain)\n",
    "    Ypred = clf.predict(Xtest)\n",
    "    score_train = clf.score(Xtrain,Ytrain)\n",
    "    print('train score %f'%score_train)\n",
    "    \n",
    "    return [Ypred,Ytest,score_train,clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "generalize_ratio = 1.0/cv\n",
    "test_ratio = 1.0/cv\n",
    "tuning_mode = False\n",
    "\n",
    "if tuning_mode:\n",
    "    cv = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TuningParametersStage1(fname=getCurrentTime()+'-stage1.csv'):\n",
    "    res = []\n",
    "    for DATAID in [3]:\n",
    "        INFO('data id %d'%DATAID)\n",
    "        for RNA_K in range(3,7):\n",
    "            for PROTEIN_K in range(3,7):\n",
    "                for TOP_RATIO in np.linspace(0.93,0.99,5):\n",
    "                    start_time = time.time()\n",
    "                    [data,T] = ReadData(DATAID,PROTEIN_K,RNA_K)\n",
    "                    [X,Y] = ToMatrix(data,'dense')\n",
    "                    \n",
    "                    for _cv in range(5): ### cv testing\n",
    "                        [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "                        [X_train,X_test,Y_train,Y_test] = \\\n",
    "                                    RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=TOP_RATIO)\n",
    "                        r = LGBTuning(X_train,X_test,Y_train,Y_test)\n",
    "                        r = {\n",
    "                            'DATAID': DATAID,\n",
    "                            'test_score':scoreFunction(r[0],r[1]),\n",
    "                            'train_score':r[2],\n",
    "                            'RNA_K':RNA_K,\n",
    "                            'PROTEIN_K':PROTEIN_K,\n",
    "                            'TOP_RATIO':TOP_RATIO,\n",
    "                            'cv':_cv,\n",
    "                        }\n",
    "                        print('DEBUG:: result ',r)\n",
    "                        res.append(r)\n",
    "                    end_time = time.time()\n",
    "                    print('DEBUG:: time elapsed ',(end_time-start_time)/60)\n",
    "    df = pd.DataFrame(data=res,columns=['DATAID','test_score','train_score','RNA_K','PROTEIN_K','TOP_RATIO'])\n",
    "    df.to_csv(os.path.join('./result',fname))\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TuningParametersStage1('data-3-global-tune-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "'''\n",
    "['NPInter10412','reRPI2825','RPI488','RPI2241','RPI1807','LPI43250','EVLncRNAs']\n",
    "'''\n",
    "params_1 = {\n",
    "    0:[(5,5,0.99)],\n",
    "    1:[(3,4,0.96)], # stage1\n",
    "    2:[(4,6,0.96)],\n",
    "    3:[(3,3,0.975),(6,3,0.93),(4,3,0.975)],\n",
    "    4:[(4,3,0.96)],\n",
    "    5:[(4,6,0.945)]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_result2 = './result/'+getCurrentTime()+'-stage2-3.csv'\n",
    "\n",
    "tune_grid = [\n",
    "    [{\n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"learning_rate\": [0.1], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [2000],\n",
    "    }],\n",
    "    [{\n",
    "        'learning_rate': [0.025,0.03,0.035,0.04], ### 0.1\n",
    "        'boosting_type':['gbdt'], ### goss>gbdt\n",
    "        'n_estimators': [500],\n",
    "        'num_iterations':[2000], ### 2000\n",
    "        'max_depth': [8,9,10], ### <400<675\n",
    "        'max_bin':[256],\n",
    "        'colsample_bytree' : [0.95,1], ### 0.75\n",
    "        'bagging_fraction':[0.9,0.95,1], ### 1\n",
    "        'bagging_freq':[1,2,3],\n",
    "        'lambda_l1': [0.075,0.1,0.125],\n",
    "    }],\n",
    "    [{\n",
    "        \"bagging_fraction\": [0.95], \n",
    "        \"bagging_freq\": [2], \n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"colsample_bytree\": [1], \n",
    "        \"lambda_l1\": [0.01], \n",
    "        \"learning_rate\": [0.2], \n",
    "        \"max_bin\": [256], \n",
    "        \"max_depth\": [7], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [2000],\n",
    "        \n",
    "#         'learning_rate': [0.15,0.2,0.25], ### 0.1\n",
    "#         'boosting_type':['gbdt'], ### goss>gbdt\n",
    "#         'n_estimators': [500],\n",
    "#         'num_iterations':[1500,2000], ### 2000\n",
    "#         'max_depth': [5,7,9], ### <400<675\n",
    "#         'max_bin':[256],\n",
    "#         'colsample_bytree' : [0.8,0.9,1], ### 0.75\n",
    "#         'bagging_fraction':[0.9,0.95,1], ### 1\n",
    "#         'bagging_freq':[2,3,4],\n",
    "#         'lambda_l1': [0.005,0.01,0.015],\n",
    "     }],\n",
    "    [{\n",
    "#         \"boosting_type\": [\"gbdt\"], \n",
    "#         \"colsample_bytree\": [0.8], \n",
    "#         \"learning_rate\": [0.05], \n",
    "#         \"max_bin\": [256], \n",
    "#         \"n_estimators\": [500], \n",
    "#         \"num_iterations\": [1000],\n",
    "        \n",
    "        'learning_rate': [0.15,0.2,0.25], ### 0.1\n",
    "        'boosting_type':['gbdt'], ### goss>gbdt\n",
    "        'n_estimators': [500],\n",
    "        'num_iterations':[2000], ### 2000\n",
    "        'max_depth': [12,13,14,15], ### <400<675\n",
    "        'max_bin':[256],\n",
    "        'colsample_bytree' : [0.7,0.8,1], ### 0.75\n",
    "        'bagging_fraction':[0.82,0.9,1], ### 1\n",
    "#         'bagging_freq':[2,3,4],\n",
    "        'lambda_l1': [0,0.01,0.02],\n",
    "     }],\n",
    "    [{\n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"learning_rate\": [0.01], \n",
    "        \"max_bin\": [256], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [2000],\n",
    "     }],\n",
    "    [{\n",
    "        \"boosting_type\": [\"gbdt\"], \n",
    "        \"learning_rate\": [0.1], \n",
    "        \"n_estimators\": [500], \n",
    "        \"num_iterations\": [1000],\n",
    "     }]\n",
    "]\n",
    "\n",
    "tune_grid = list(map(lambda x:list(ParameterGrid(x)),tune_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuning_cv2 = 5\n",
    "tuning_generalize_ratio2 = 1.0/tuning_cv2 if tuning_cv2!=1 else 0.2\n",
    "\n",
    "df_columns = ['dataid','protein_k','rna_k','top_ratio','training_score','tune_param','scores','cv']\n",
    "df_result2 = pd.DataFrame([],columns=df_columns)\n",
    "\n",
    "count = 0\n",
    "for _dataid in [3]:\n",
    "    INFO('dataid %d'%_dataid)\n",
    "    for global_params in params_1[_dataid]:\n",
    "        \n",
    "        ### get conf of dataid\n",
    "        protein_k = global_params[0]\n",
    "        rna_k = global_params[1]\n",
    "        top_ratio = global_params[2]\n",
    "        ### read data\n",
    "        [data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "\n",
    "        for _cv in range(tuning_cv2):\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            INFO('tuning cv %d'%_cv)\n",
    "            \n",
    "            ### split dataset\n",
    "            [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,tuning_generalize_ratio2)\n",
    "            ### dimensionality reduction\n",
    "            [X_train,X_test,Y_train,Y_test] = \\\n",
    "                        RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "            \n",
    "            for sp in tune_grid[_dataid]:\n",
    "                sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "                tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp)\n",
    "                tune_score = scoreFunction(tune_results[0],tune_results[1])\n",
    "                r = pd.Series({\n",
    "                                'dataid':_dataid,\n",
    "                                'protein_k':protein_k,\n",
    "                                'rna_k':rna_k,\n",
    "                                'top_ratio':top_ratio,\n",
    "                                'training_score':tune_results[2],\n",
    "                                'tune_param':json.dumps(sp),\n",
    "                                'scores':json.dumps(tune_score),\n",
    "                                'cv':_cv\n",
    "                })\n",
    "                df_result2 = df_result2.append(r,ignore_index=True)\n",
    "                count += 1\n",
    "                if count%100==1:\n",
    "                    print(dict(r))\n",
    "    \n",
    "            end_time = time.time()\n",
    "            print('DEBUG:: time elapsed ',(end_time-start_time)/60)\n",
    "df_result2.to_csv(fname_result2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data 3 3\n",
      "=============RETRIEVE TRIAN DATA=================\n",
      "# DEBUG: # DEBUG: **************new dl 3***************\n",
      "# DEBUG: READ SEQ FROM FILE\n",
      "# DEBUG: READ CLUSTER FROM FILE\n",
      "ERROR:: regex  \n",
      "ERROR:: regex  \n",
      "# DEBUG: READ PAIR FROM FILE\n",
      "# DEBUG: GENERATE NEGATIVE PAIR\n",
      "# DEBUG: negative pair number 2240\n",
      "INFO::count of negative pairs2240\n",
      "# DEBUG: PAIR UNION\n",
      "# DEBUG: EXTRACT FEATURES--PROTEIN\n",
      "# DEBUG: EXTRACT FEATURES--RNA\n",
      "# DEBUG: K-MER CALCULATION\n",
      "# DEBUG: FEATURE UNION\n",
      "# DEBUG: GARBAGE COLLECTION\n",
      "MATRIX TRANSFORMATION\n",
      "DEBUG:: total features count  2375\n",
      "data shape 4480 2375\n",
      "rf raw data fit score 0.999721\n",
      "INFO::dimension remained 2344 0.975000\n",
      "dimension remained 2344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\optuna\\_experimental.py:111: ExperimentalWarning: RedisStorage is experimental (supported from v1.4.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n",
      "[I 2020-05-25 18:15:00,582] Finished trial#0 with value: 0.7879464285714286 with parameters: {'solver': 'adam', 'learning_rate_init': 0.008764598659058494, 'hidden_layer_sizes_1': 600.0, 'hidden_layer_sizes_2': 200.0, 'hidden_layer_sizes_f': 30.0, 'max_iter': 1000.0, 'alpha': 0.015000216978308504}. Best is trial#0 with value: 0.7879464285714286.\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n",
      "[I 2020-05-25 18:16:39,615] Finished trial#1 with value: 0.7734375 with parameters: {'solver': 'adam', 'learning_rate_init': 0.00885509443649648, 'hidden_layer_sizes_1': 820.0, 'hidden_layer_sizes_2': 200.0, 'hidden_layer_sizes_f': 30.0, 'max_iter': 500.0, 'alpha': 0.9366619459828381}. Best is trial#0 with value: 0.7879464285714286.\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n",
      "[I 2020-05-25 18:18:03,851] Finished trial#2 with value: 0.8370535714285714 with parameters: {'solver': 'adam', 'learning_rate_init': 0.00027003285183822406, 'hidden_layer_sizes_1': 540.0, 'hidden_layer_sizes_2': 220.0, 'hidden_layer_sizes_f': 10.0, 'max_iter': 900.0, 'alpha': 0.000479006493235985}. Best is trial#2 with value: 0.8370535714285714.\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n",
      "[I 2020-05-25 18:20:09,777] Finished trial#3 with value: 0.8113839285714286 with parameters: {'solver': 'adam', 'learning_rate_init': 0.0006619976040305931, 'hidden_layer_sizes_1': 880.0, 'hidden_layer_sizes_2': 180.0, 'hidden_layer_sizes_f': 20.0, 'max_iter': 600.0, 'alpha': 0.10111937474570243}. Best is trial#2 with value: 0.8370535714285714.\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "[I 2020-05-25 18:24:46,765] Finished trial#4 with value: 0.8169642857142857 with parameters: {'solver': 'adam', 'learning_rate_init': 3.2409399153423836e-05, 'hidden_layer_sizes_1': 420.0, 'hidden_layer_sizes_2': 230.0, 'hidden_layer_sizes_f': 20.0, 'max_iter': 300.0, 'alpha': 0.09902313154104372}. Best is trial#2 with value: 0.8370535714285714.\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n",
      "[I 2020-05-25 18:25:36,754] Finished trial#5 with value: 0.8203125 with parameters: {'solver': 'adam', 'learning_rate_init': 0.005058265960060126, 'hidden_layer_sizes_1': 510.0, 'hidden_layer_sizes_2': 340.0, 'hidden_layer_sizes_f': 50.0, 'max_iter': 900.0, 'alpha': 0.00016378031312096992}. Best is trial#2 with value: 0.8370535714285714.\n",
      "D:\\anaconda\\lib\\site-packages\\optuna\\trial.py:1061: RuntimeWarning: Inconsistent parameter values for distribution with name \"max_iter\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 200, 'high': 1000, 'q': 100}\n",
      "  RuntimeWarning,\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    param = {\n",
    "        'objective': 'cross_entropy',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate', 1e-2, 0.5),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-15, 1),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-15, 4),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 1024),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 25),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 3, 30),\n",
    "        \n",
    "        'n_estimators': 500,\n",
    "        'num_iterations': 1000,\n",
    "        'max_bin': 256,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': 20,\n",
    "    }\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train,Y_train)\n",
    "    clf = lgb.train(param, dtrain)\n",
    "    \n",
    "    Ypred_test = clf.predict(X_test)\n",
    "    Ypred_test = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_test)))\n",
    "    \n",
    "    Ypred_train = clf.predict(X_train)\n",
    "    Ypred_train = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_train)))\n",
    "    \n",
    "    score_test = scoreFunction(Ypred_test,Y_test)\n",
    "    score_train = scoreFunction(Ypred_train,Y_train)\n",
    "    \n",
    "    trial.set_user_attr('test_scores', score_test)\n",
    "    trial.set_user_attr('train_scores', score_train)\n",
    "    \n",
    "    return score_test['acc']\n",
    "\n",
    "def objective_nn(trial):\n",
    "\n",
    "    param = {\n",
    "        'solver': trial.suggest_categorical('solver', ['adam']),\n",
    "        'learning_rate_init': trial.suggest_loguniform('learning_rate_init', 1e-5, 0.05),\n",
    "        'hidden_layer_sizes_1': int(trial.suggest_discrete_uniform('hidden_layer_sizes_1', 300, 1000, 10)),\n",
    "        'hidden_layer_sizes_2': int(trial.suggest_discrete_uniform('hidden_layer_sizes_2', 100, 400, 10)),\n",
    "        'hidden_layer_sizes_f': int(trial.suggest_discrete_uniform('hidden_layer_sizes_f', 10, 50, 10)),\n",
    "#         'hidden_layer_sizes_3': int(trial.suggest_discrete_uniform('hidden_layer_sizes', 50, 1000, 10)),\n",
    "        'max_iter': int(trial.suggest_discrete_uniform('max_iter', 200, 1000, 100)),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-5, 1),\n",
    "#         'activation': trial.suggest_categorical('activation', ['tanh','relu']),\n",
    "#         'beta_1': trial.suggest_loguniform('learning_rate_init', 1e-5, 0.05),\n",
    "        'batch_size': int(trial.suggest_discrete_uniform('max_iter', 50, 300, 10)),\n",
    "        'max_bin': 256,\n",
    "        'random_state': seed,\n",
    "        'n_jobs': 20,\n",
    "    }\n",
    "    \n",
    "    clf = MLPClassifier(random_state=param['random_state'], \n",
    "                        max_iter=param['max_iter'],\n",
    "                        hidden_layer_sizes=(param['hidden_layer_sizes_1'],param['hidden_layer_sizes_2'],param['hidden_layer_sizes_f']),\n",
    "                        learning_rate_init=param['learning_rate_init'],\n",
    "                        solver=param['solver'],\n",
    "#                         activation=param['activation'],\n",
    "                        alpha=param['alpha'],\n",
    "                        batch_size=param['batch_size'],\n",
    "                       )\n",
    "    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    Ypred_test = clf.predict(X_test)\n",
    "    Ypred_test = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_test)))\n",
    "    \n",
    "    Ypred_train = clf.predict(X_train)\n",
    "    Ypred_train = np.array(list(map(lambda x:0 if x<=0.5 else 1,Ypred_train)))\n",
    "    \n",
    "    score_test = scoreFunction(Ypred_test,Y_test)\n",
    "    score_train = scoreFunction(Ypred_train,Y_train)\n",
    "    \n",
    "    trial.set_user_attr('test_scores', score_test)\n",
    "    trial.set_user_attr('train_scores', score_train)    \n",
    "    \n",
    "    return score_test['acc']\n",
    "    \n",
    "_dataid = 3\n",
    "global_params = params_1[_dataid][0]\n",
    "### get conf of dataid\n",
    "protein_k = global_params[0]\n",
    "rna_k = global_params[1]\n",
    "top_ratio = global_params[2]\n",
    "### read data\n",
    "[data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "[X,Y] = ToMatrix(data,'dense')\n",
    "### split dataset\n",
    "[X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,0.2)\n",
    "### dimensionality reduction\n",
    "[X_train,X_test,Y_train,Y_test] = \\\n",
    "            RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "\n",
    "storage = optuna.storages.redis.RedisStorage(\n",
    "    url='redis://QAZPLMTgv@123@r-bp1ba71b01eb1a94pd.redis.rds.aliyuncs.com:6379/db1',\n",
    ")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_nn, n_trials=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optuna.visualization.plot_intermediate_values(study)\n",
    "# optuna.visualization.plot_optimization_history(study)\n",
    "study.best_trial.params\n",
    "study.best_trial.user_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "r = redis.StrictRedis(host='r-bp1ba71b01eb1a94pd.redis.rds.aliyuncs.com', port=6379, db=1,password='QAZPLMTgv@123')\n",
    "r.hmset(getCurrentTime()+'@'+str(_dataid),{\n",
    "    \"dataset\":str(_dataid),\n",
    "    \"params\":json.dumps(study.best_trial.params),\n",
    "    \"train-scores\":json.dumps(study.best_trial.user_attrs['train_scores']),\n",
    "    \"test-scores\":json.dumps(study.best_trial.user_attrs['test_scores']),\n",
    "          })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_cv2 = 5\n",
    "tuning_generalize_ratio2 = 1.0/tuning_cv2 if tuning_cv2!=1 else 0.2\n",
    "\n",
    "df_columns = ['dataid','protein_k','rna_k','top_ratio','training_score','tune_param','scores']\n",
    "df_result2 = pd.DataFrame([],columns=df_columns)\n",
    "\n",
    "for _dataid in [0,1,2,3,4,5]:\n",
    "    INFO('dataid %d'%_dataid)\n",
    "    for global_params in params_1[_dataid]:\n",
    "        start_time = time.time()\n",
    "        ### get conf of dataid\n",
    "        protein_k = global_params[0]\n",
    "        rna_k = global_params[1]\n",
    "        top_ratio = global_params[2]\n",
    "        ### read data\n",
    "        [data,T] = ReadData(_dataid,protein_k,rna_k)\n",
    "        [X,Y] = ToMatrix(data,'dense')\n",
    "\n",
    "        for _cv in range(tuning_cv2):\n",
    "            INFO('tuning cv %d'%_cv)\n",
    "            \n",
    "            ### split dataset\n",
    "            [X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,tuning_generalize_ratio2)\n",
    "            ### dimensionality reduction\n",
    "            [X_train,X_test,Y_train,Y_test] = \\\n",
    "                        RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=top_ratio)\n",
    "            \n",
    "            for sp in tune_grid[_dataid]:\n",
    "                sp = dict(map(lambda x:(x,[sp[x]]),sp))\n",
    "                tune_results = LGBTuning(X_train,X_test,Y_train,Y_test,sp)\n",
    "                tune_score = scoreFunction(tune_results[0],tune_results[1])\n",
    "                r = pd.Series({\n",
    "                                'dataid':_dataid,\n",
    "                                'protein_k':protein_k,\n",
    "                                'rna_k':rna_k,\n",
    "                                'top_ratio':top_ratio,\n",
    "                                'training_score':tune_results[2],\n",
    "                                'tune_param':json.dumps(sp),\n",
    "                                'scores':json.dumps(tune_score),\n",
    "                })\n",
    "                df_result2 = df_result2.append(r,ignore_index=True)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        print('DEBUG:: time elapsed ',(end_time-start_time)/60)\n",
    "df_result2.to_csv(fname_result2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_df_from_stage2_result(fpath,group=['dataid']):\n",
    "    df = pd.read_csv(fpath)\n",
    "    acc = [json.loads(row['scores'])['acc'] for idx,row in df.iterrows()]\n",
    "    auc = [json.loads(row['scores'])['auc'] for idx,row in df.iterrows()]\n",
    "    fpr = [json.loads(row['scores'])['fpr'] for idx,row in df.iterrows()]\n",
    "    tpr = [json.loads(row['scores'])['tpr'] for idx,row in df.iterrows()]\n",
    "    mcc = [json.loads(row['scores'])['mcc'] for idx,row in df.iterrows()]\n",
    "    tnr = [json.loads(row['scores'])['tnr'] for idx,row in df.iterrows()]\n",
    "    ppv = [json.loads(row['scores'])['ppv'] for idx,row in df.iterrows()]\n",
    "    f_score = [json.loads(row['scores'])['f_score'] for idx,row in df.iterrows()]\n",
    "    ap = [json.loads(row['scores'])['ap'] for idx,row in df.iterrows()]\n",
    "    brier = [json.loads(row['scores'])['brier'] for idx,row in df.iterrows()]\n",
    "    sensitivity = [json.loads(row['scores'])['sensitivity'] for idx,row in df.iterrows()]\n",
    "#     print(fpr,tpr)\n",
    "    df['acc'] = acc\n",
    "    df['auc'] = auc\n",
    "    df['fpr'] = np.array(fpr)[:,1]\n",
    "    df['tpr'] = np.array(tpr)[:,1]\n",
    "    df['mcc'] = mcc\n",
    "    df['tnr'] = tnr\n",
    "    df['ppv'] = ppv\n",
    "    df['f_score'] = f_score\n",
    "    df['ap'] = ap\n",
    "    df['brier'] = brier\n",
    "    df['sensitivity'] = sensitivity\n",
    "\n",
    "    a = df.groupby(by=group).agg({\n",
    "        'acc':np.mean,\n",
    "        'auc':np.mean,\n",
    "        'fpr':np.mean,\n",
    "        'tpr':np.mean,\n",
    "        'mcc':np.mean,\n",
    "        'tnr':np.mean,\n",
    "        'ppv':np.mean,\n",
    "        'f_score':np.mean,\n",
    "        'ap':np.mean,\n",
    "        'brier':np.mean,\n",
    "        'sensitivity':np.mean,\n",
    "    }).reset_index()\n",
    "#     b = a.join(df.set_index(['dataid','acc']),on=['dataid','acc'],how='inner',lsuffix='_left', rsuffix='_right')\n",
    "    return a\n",
    "\n",
    "# df_tune1 = get_df_from_stage2_result('./result2020-05-09-16-22-10-stage2.csv')\n",
    "# df_sub2_tune1 = get_df_from_stage2_result('./result2020-05-09-21-08-01-stage2.csv')\n",
    "# df_raw = get_df_from_stage2_result('./result2020-05-10-10-30-30-stage2.csv')\n",
    "# df_tune = get_df_from_stage2_result('./result2020-05-09-21-42-23-stage2.csv')\n",
    "\n",
    "df = get_df_from_stage2_result('./result/2020-05-13-21-28-20-stage2-3.csv',\n",
    "                               group=['dataid','protein_k','rna_k','top_ratio','tune_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[ (df['acc']==df['acc'].max()) | (df['auc']==df['auc'].max()) ].values\n",
    "# df['acc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['RNA_K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tune1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub2_tune1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_optimal_stage1 = './result/2020-05-04-20-55-26-stage1.csv'\n",
    "df = pd.read_csv(fname_optimal_stage1)\n",
    "acc = [ float(re.findall('[0-9]+\\.[0-9]+',x)[0]) for x in df['test_score'] ]\n",
    "auc = [ float(re.findall('[0-9]+\\.[0-9]+',x)[1]) for x in df['test_score'] ]\n",
    "df['acc'] = acc\n",
    "df['auc'] = auc\n",
    "\n",
    "new_df1 = df.groupby(by=['DATAID','PROTEIN_K','RNA_K']).agg({'acc':np.max,'auc':np.max})\n",
    "new_df2 = df.groupby(by=['DATAID','PROTEIN_K','RNA_K']).agg({'acc':np.mean,'auc':np.mean})\n",
    "inspect_df = df.groupby(by=['DATAID']).agg({'acc':np.max,'auc':np.max})\n",
    "inspect_df.join(df.set_index(['DATAID','acc']),on=['DATAID','acc'],how='inner',lsuffix='_left', rsuffix='_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [json.loads(row['scores'])['acc'] for idx,row in df_result2.iterrows()]\n",
    "auc = [json.loads(row['scores'])['auc'] for idx,row in df_result2.iterrows()]\n",
    "\n",
    "df_result2['acc'] = acc\n",
    "df_result2['auc'] = auc\n",
    "\n",
    "df_result2.groupby(by=['dataid']).agg({'acc':np.mean})\n",
    "# df_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data 3 3\n",
      "=============RETRIEVE TRIAN DATA=================\n",
      "# DEBUG: # DEBUG: **************new dl 3***************\n",
      "# DEBUG: READ SEQ FROM FILE\n",
      "# DEBUG: READ CLUSTER FROM FILE\n",
      "ERROR:: regex  \n",
      "ERROR:: regex  \n",
      "# DEBUG: READ PAIR FROM FILE\n",
      "# DEBUG: GENERATE NEGATIVE PAIR\n",
      "# DEBUG: negative pair number 2240\n",
      "INFO::count of negative pairs2240\n",
      "# DEBUG: PAIR UNION\n",
      "# DEBUG: EXTRACT FEATURES--PROTEIN\n",
      "# DEBUG: EXTRACT FEATURES--RNA\n",
      "# DEBUG: K-MER CALCULATION\n",
      "# DEBUG: FEATURE UNION\n",
      "# DEBUG: GARBAGE COLLECTION\n",
      "MATRIX TRANSFORMATION\n",
      "DEBUG:: total features count  2375\n",
      "data shape 4480 2375\n",
      "rf raw data fit score 0.999721\n",
      "INFO::dimension remained 2344 0.975000\n",
      "dimension remained 2344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.999721\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "DATAID = 3\n",
    "PROTEIN_K = params_1[DATAID][0][0]\n",
    "RNA_K = params_1[DATAID][0][1]\n",
    "topRatio = params_1[DATAID][0][2]\n",
    "[data,T] = ReadData(DATAID,PROTEIN_K,RNA_K)\n",
    "[X,Y] = ToMatrix(data,'dense')\n",
    "[X_train,X_test,Y_train,Y_test] = SplitDataset(X,Y,generalize_ratio)\n",
    "[X_train,X_test,Y_train,Y_test] = \\\n",
    "    RandomForestDimensionalityReduction(X_train,X_test,Y_train,Y_test,topRatio=topRatio)\n",
    "r1 = LGBTuning(X_train,X_test,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimators = [\n",
    "#     ('cb', Tr.DecisionTreeClassifier()),\n",
    "#     ('lgb'+str(i), lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=i)) for i in range(5,20)\n",
    "#     ('lgb2', lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=6)),\n",
    "#     ('lgb3', lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=3)),\n",
    "#     ('lgb4', lgb.LGBMClassifier(objective='cross_entropy', random_state=seed, n_jobs=6,max_depth=12)),\n",
    "#     ('xgb',xgb.XGBClassifier()),\n",
    "# ]\n",
    "# clf = StackingClassifier(\n",
    "#     estimators=estimators, final_estimator=LogisticRegressionCV(cv=10)\n",
    "# )\n",
    "clf = BaggingClassifier(base_estimator=MLPClassifier(solver= 'adam',\n",
    "                    learning_rate_init= 0.0008421977409048232,\n",
    "                    hidden_layer_sizes= (550,128),\n",
    "                    max_iter= 300,\n",
    "                    alpha= 0.4891603457998322,\n",
    "                    activation= 'relu'\n",
    "                   ),\n",
    "                        max_samples=0.7,max_features=0.6,bootstrap_features=True,\n",
    "                         n_estimators=50, random_state=0, n_jobs=-1).fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8102678571428571,\n",
       " 'auc': 0.8102630831127263,\n",
       " 'fpr': [0.0, 0.1824175824175824, 1.0],\n",
       " 'tpr': [0.0, 0.8027210884353742, 1.0],\n",
       " 'mcc': 0.6204148261328282,\n",
       " 'tnr': 0.8175824175824176,\n",
       " 'ppv': 0.8100686498855835,\n",
       " 'f_score': 0.806378132118451,\n",
       " 'ap': 0.7473574026292439,\n",
       " 'brier': 0.18973214285714285,\n",
       " 'sensitivity': 0.8027210884353742}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = MLPClassifier(solver= 'adam',\n",
    "#                     learning_rate_init= 0.0008421977409048232,\n",
    "#                     hidden_layer_sizes_1= 930.0,\n",
    "#                     hidden_layer_sizes_2= 250.0,\n",
    "#                     max_iter= 300.0,\n",
    "#                     alpha= 0.4891603457998322,\n",
    "#                     activation= 'relu'\n",
    "#                    ).fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)\n",
    "scoreFunction(Y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9793526785714286,\n",
       " 'auc': 0.9793772919440356,\n",
       " 'fpr': [0.0, 0.023529411764705882, 1.0],\n",
       " 'tpr': [0.0, 0.9822123401889938, 1.0],\n",
       " 'mcc': 0.9587187554867308,\n",
       " 'tnr': 0.9764705882352941,\n",
       " 'ppv': 0.9767827529021559,\n",
       " 'f_score': 0.979490022172949,\n",
       " 'ap': 0.9683366450128457,\n",
       " 'brier': 0.020647321428571428,\n",
       " 'sensitivity': 0.9822123401889938}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ypred = clf.predict(X_train)\n",
    "scoreFunction(Ypred,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study_name = 'example-study'  # Unique identifier of the study.\n",
    "study = optuna.create_study(study_name=study_name, \n",
    "                            storage='sqlite:///example.db',\n",
    "                            load_if_exists=True,\n",
    "                            pruner=optuna.pruners.MedianPruner()\n",
    "                           )\n",
    "\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))\n",
    "\n",
    "# study.best_params  # Get best parameters for the objective function.\n",
    "# study.best_value  # Get best objective value.\n",
    "# study.best_trial  # Get best trial's information.\n",
    "# study.trials  # Get all trials' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, 2344)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
